============ PROMPT GIVEN TO PERPLEXITY ===================

<Context>
I have to undertake Project 2 for the Tools in Data Science course, wherein I basically have to make a data analyst agent. So any job that a Data Analyst would be given by the manager, should be able to be done by the agent. The broad flow I envision is that we have one LLM that reads the question, and writes code to undertake that task, then another LLM that looks at the requirement and evaluated LLM1's code (and maybe improves on it?) , and then finally LLM3 or a loop, that makes LLM execute the code at hand for requirement. Use cases could range from RAG things (readings legal documents, pdfs, csv files etc), then web scraping and answering something specific, processing png images as part of the prompt, and maybe other cases that a Data Analyst does. Now consideration needs to be kept in mind of the libraries to use (or will the agent be able to install them as per requirement.
<Project Details from course>
[Project: Data Analyst Agent](https://tds.s-anand.net/#/project-data-analyst-agent?id=project-data-analyst-agent)
[!WARNING] > THIS IS A DRAFT
Deploy a data analyst agent. This is an API that uses LLMs to source, prepare, analyze, and visualize any data.
Your application exposes an API endpoint. You may host it anywhere. Let’s assume it’s at https://app.example.com/api/.
The endpoint must accept a POST request, e.g. POST https://app.example.com/api/ with a data analysis task description in the body. For example:
curl "https://app.example.com/api/" -F "@question.txt"Copy to clipboardErrorCopied
The answers must be sent within 3 minutes in the format requested.
[Sample questions](https://tds.s-anand.net/#/project-data-analyst-agent?id=sample-questions)
These are examples of quesions.txt that will be sent. NOT the actual question, which will be a secret.
Scrape the list of highest grossing films from Wikipedia. It is at the URL:
https://en.wikipedia.org/wiki/List_of_highest-grossing_films

Answer the following questions and respond with a JSON array of strings containing the answer.

1. How many $2 bn movies were released before 2000?
2. Which is the earliest film that grossed over $1.5 bn?
3. What's the correlation between the Rank and Peak?
4. Draw a scatterplot of Rank and Peak along with a dotted red regression line through it.
Return as a base-64 encoded data URI, `"data:image/png;base64,iVBORw0KG..."` under 100,000 bytes.Copy to clipboardErrorCopied
Here’s another example.
The Indian high court judgement dataset contains judgements from the Indian High Courts, downloaded from [ecourts website](https://judgments.ecourts.gov.in/). It contains judgments of 25 high courts, along with raw metadata (as .json) and structured metadata (as .parquet).

- 25 high courts
- ~16M judgments
- ~1TB of data

Structure of the data in the bucket:

- `data/pdf/year=2025/court=xyz/bench=xyz/judgment1.pdf,judgment2.pdf`
- `metadata/json/year=2025/court=xyz/bench=xyz/judgment1.json,judgment2.json`
- `metadata/parquet/year=2025/court=xyz/bench=xyz/metadata.parquet`
- `metadata/tar/year=2025/court=xyz/bench=xyz/metadata.tar.gz`
- `data/tar/year=2025/court=xyz/bench=xyz/pdfs.tar`

This DuckDB query counts the number of decisions in the dataset.

```sql
INSTALL httpfs; LOAD httpfs;
INSTALL parquet; LOAD parquet;

SELECT COUNT(*) FROM read_parquet('s3://indian-high-court-judgments/metadata/parquet/year=*/court=*/bench=*/metadata.parquet?s3_region=ap-south-1');
```

Here are the columns in the data:

| Column | Type | Description |
| ---------------------- | ------- | ------------------------------ |
| `court_code` | VARCHAR | Court identifier (e.g., 33~10) |
| `title` | VARCHAR | Case title and parties |
| `description` | VARCHAR | Case description |
| `judge` | VARCHAR | Presiding judge(s) |
| `pdf_link` | VARCHAR | Link to judgment PDF |
| `cnr` | VARCHAR | Case Number Register |
| `date_of_registration` | VARCHAR | Registration date |
| `decision_date` | DATE | Date of judgment |
| `disposal_nature` | VARCHAR | Case outcome |
| `court` | VARCHAR | Court name |
| `raw_html` | VARCHAR | Original HTML content |
| `bench` | VARCHAR | Bench identifier |
| `year` | BIGINT | Year partition |

Here is a sample row:

```json
{
"court_code": "33~10",
"title": "CRL MP(MD)/4399/2023 of Vinoth Vs The Inspector of Police",
"description": "No.4399 of 2023 BEFORE THE MADURAI BENCH OF MADRAS HIGH COURT ( Criminal Jurisdiction ) Thursday, ...",
"judge": "HONOURABLE MR JUSTICE G.K. ILANTHIRAIYAN",
"pdf_link": "court/cnrorders/mdubench/orders/HCMD010287762023_1_2023-03-16.pdf",
"cnr": "HCMD010287762023",
"date_of_registration": "14-03-2023",
"decision_date": "2023-03-16",
"disposal_nature": "DISMISSED",
"court": "33_10",
"raw_html": "<button type='button' role='link'..",
"bench": "mdubench",
"year": 2023
}
```

Answer the following questions and respond with a JSON object containing the answer.

```json
{
"Which high court disposed the most cases from 2019 - 2022?": "...",
"What's the regression slope of the date_of_registration - decision_date by year in the court=33_10?": "...",
"Plot the year and # of days of delay from the above question as a scatterplot with a regression line. Encode as a base64 data URI under 100,000 characters": "data:image/webp:base64,..."
}
```Copy to clipboardErrorCopied
[Sample responses](https://tds.s-anand.net/#/project-data-analyst-agent?id=sample-responses)
Here is a sample response to the first question:
[1, "Titanic", 0.485782, "data:image/png;base64,iVBORw0KG... (response truncated)"]Copy to clipboardErrorCopied
[Evaluation](https://tds.s-anand.net/#/project-data-analyst-agent?id=evaluation)
Here is a sample evaluation to the first question. This is indicative. The real evaluation will depend on the actual question, which will be a secret.
description: "TDS Data Analyst Agent – generic eval (20-point rubric)"

providers:
- id: https
config:
url: https://app.example.com/api/ # Replace this with your API endpoint
method: POST
body: file://question.txt
transformResponse: json

assert:
# Structural gate – no score, hard-fail if not a 4-element array
- type: is-json
value: {type: array, minItems: 4, maxItems: 4}
weight: 0

# 1️⃣ first answer must equal 1
- type: python
weight: 4
value: |
import json, sys
print(json.loads(output)[0] == 1)

# 2️⃣ second answer must contain “Titanic” (case-insensitive)
- type: python
weight: 4
value: |
import json, re, sys
print(bool(re.search(r'titanic', json.loads(output)[1], re.I)))

# 3️⃣ third answer within ±0.001 of 0.485782
- type: python
weight: 4
value: |
import json, sys, math
print(abs(float(json.loads(output)[2]) - 0.485782) <= 0.001)

# 4️⃣ vision check ― send plot to GPT-4o-mini and grade multiple criteria
- type: llm-rubric
provider: openai:gpt-4.1-nano
weight: 8
# extract base-64 PNG from the 4th array element and inject into the prompt
preprocess: |
import json, re
data = json.loads(output)
context['plot'] = data[3
rubricPrompt: |
[
{ "role": "system",
"content": "Grade the scatterplot. Award *score 1* only iff ALL are true: \
(a) it’s a scatterplot of Rank (x-axis) vs Peak (y-axis); \
(b) a dotted **red** regression line is present; \
(c) axes are visible & labelled; \
(d) file size < 100 kB. Otherwise score 0. \
Respond as JSON: {scatterplot:bool, regression:bool, axes:bool, size:bool, score:number}"
},
{ "role": "user",
"content": [
{ "type": "image_url",
"image_url": { "url": "{{plot}}" } # data:image/png;base64,… :contentReference[oaicite:5]{index=5}
},
{ "type": "text",
"text": "Here is the original task:\n\n{{vars.question}}\n\nReview the image and JSON above." }
]
}
]
threshold: 0.99 # require full pass

tests:
- description: "Data analysis"Copy to clipboardErrorCopied
Your score will be the score provided by promptfoo. No normalization. What you get is what you get.
[Deploy your application](https://tds.s-anand.net/#/project-data-analyst-agent?id=deploy-your-application)
Deploy your application to a public URL that can be accessed by anyone. You may use any platform.
(If you use ngrok, ensure that it is running continuously until you get your results.)
[Share your code](https://tds.s-anand.net/#/project-data-analyst-agent?id=share-your-code)
[Create a new public GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)
[Add an MIT LICENSE file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository)
Commit and push your code
[Submit your URL](https://tds.s-anand.net/#/project-data-analyst-agent?id=submit-your-url)
Submit your GitHub repository URL and your API endpoint URL at [https://exam.sanand.workers.dev/tds-data-analyst-agent](https://exam.sanand.workers.dev/tds-data-analyst-agent) (once it is available).
< What i Already Have>
I am working in the folder: (.venv) PS C:\Users\samar\OneDrive\Desktop\TDS_DATA_ANALYST_AGENT>
<some basic code>
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import os
from dotenv import load_dotenv
from pydantic import BaseModel


load_dotenv()
AI_PIPE = os.getenv("AI_PIPE")
print(f"AI_PIPE TOKEN: {AI_PIPE[:10]}")


#Make a fastapi app
app = FastAPI()
# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)
< My requirement from you>
Is that we go very slow, step by step and test things. I have an AI pipe token which can be used (but should allow scraping, pdf, base64 images and so on). Ask me for anything more or less you need from me. Maybe we can think about using MCP ( i dont know if its relevant here, but maybe to make if any files accessible if in question). I also want to learn a lot in the process (using pydantic base model , with comments of how it works). I need to understand what each building block does basically.
Let's start smartly and slowly with reasoning.






===================== FOR ADDITIONAL FILES ==============
OKay now i need help restructuring. This html thing involving web scraping and answering is more or less working. Now if extra files are passed like so:
<Project Desc>
[Project: Data Analyst Agent](https://tds.s-anand.net/#/project-data-analyst-agent?id=project-data-analyst-agent)
Deploy a data analyst agent. This is an API that uses LLMs to source, prepare, analyze, and visualize any data.
Your application exposes an API endpoint. You may host it anywhere. Let’s assume it’s at https://app.example.com/api/.
The endpoint must accept a POST request, e.g. POST https://app.example.com/api/ with a data analysis task description and optional attachments in the body. For example:
curl "https://app.example.com/api/" -F "questions.txt=@question.txt" -F "image.png=@image.png" -F "data.csv=@data.csv"Copy to clipboardErrorCopied
questions.txt will ALWAYS be sent and contain the questions. There may be zero or more additional files passed.
The answers must be sent within 3 minutes in the format requested.
</Project Desc>
<My current untested Code>

import os, tempfile, json
import pandas as pd
from io import StringIO
import pdfplumber
import os, tempfile, tarfile, zipfile
from typing import List
from fastapi import UploadFile, File


# ============== PREVIEW FROM ATTACHED FILES ========================
def try_preview_from_files(files: List[UploadFile]):
    for file in files:
        try:
            filename = file.filename.lower()


            # === CSV/TSV ===
            if filename.endswith(".csv") or filename.endswith(".tsv"):
                df = pd.read_csv(file.file, sep='\t' if filename.endswith(".tsv") else ',')
                return {
                    "source": filename,
                    "columns": df.columns.tolist(),
                    "sample_rows": df.head(5).to_dict(orient="records")
                }


            # === Excel ===
            elif filename.endswith(".xlsx"):
                df = pd.read_excel(file.file)
                return {
                    "source": filename,
                    "columns": df.columns.tolist(),
                    "sample_rows": df.head(5).to_dict(orient="records")
                }


            # === JSON ===
            elif filename.endswith(".json"):
                content = file.file.read().decode("utf-8")
                try:
                    df = pd.read_json(StringIO(content), lines=True)
                except:
                    df = pd.json_normalize(json.loads(content))
                return {
                    "source": filename,
                    "columns": df.columns.tolist(),
                    "sample_rows": df.head(5).to_dict(orient="records")
                }


            # === Parquet ===
            elif filename.endswith(".parquet"):
                df = pd.read_parquet(file.file)
                return {
                    "source": filename,
                    "columns": df.columns.tolist(),
                    "sample_rows": df.head(3).to_dict(orient="records")
                }


            # === HTML ===
            elif filename.endswith(".html"):
                content = file.file.read().decode("utf-8")
                tables = pd.read_html(StringIO(content))
                if tables:
                    df = tables[0]
                    return {
                        "source": filename,
                        "columns": df.columns.tolist(),
                        "sample_rows": df.head(3).to_dict(orient="records")
                    }


            # === PDF ===
            elif filename.endswith(".pdf"):
                with pdfplumber.open(file.file) as pdf:
                    text = "\n".join([page.extract_text() or '' for page in pdf.pages])
                return {
                    "source": filename,
                    "columns": ["text"],
                    "sample_rows": [{"text": text[:500]}]
                }


            # === TAR / TAR.GZ / ZIP ===
            elif filename.endswith((".tar.gz", ".tar", ".zip")):
                with tempfile.TemporaryDirectory() as tmpdir:
                    archive_path = os.path.join(tmpdir, file.filename)
                    with open(archive_path, "wb") as f:
                        f.write(file.file.read())


                    if filename.endswith(".zip"):
                        with zipfile.ZipFile(archive_path, 'r') as zf:
                            zf.extractall(tmpdir)
                    else:
                        with tarfile.open(archive_path, "r:*") as tar:
                            tar.extractall(tmpdir)


                    # Try to preview any extracted file
                    for root, _, files_in_dir in os.walk(tmpdir):
                        for f in files_in_dir:
                            ext = f.lower()
                            fpath = os.path.join(root, f)
                            try:
                                if ext.endswith(".csv"):
                                    df = pd.read_csv(fpath)
                                elif ext.endswith(".json"):
                                    try:
                                        df = pd.read_json(fpath, lines=True)
                                    except:
                                        with open(fpath, 'r') as j:
                                            df = pd.json_normalize(json.load(j))
                                elif ext.endswith(".parquet"):
                                    df = pd.read_parquet(fpath)
                                elif ext.endswith(".xlsx"):
                                    df = pd.read_excel(fpath)
                                else:
                                    continue


                                return {
                                    "source": fpath,
                                    "columns": df.columns.tolist(),
                                    "sample_rows": df.head(5).to_dict(orient="records")
                                }
                            except:
                                continue


        except Exception as e:
            continue  # Skip bad files


    return None  # Nothing previewable found
========== in main_anthropic.py ==============
# === Main Endpoint ===
@app.post("/api/")
async def analyze(request: Request):
    stdout = ""
    stderr = ""
    try:
        form = await request.form()
        question_file = form.get("questions.txt")
        if not question_file:
            return {"error": "Missing 'questions.txt' in request."}


        question_text = (await question_file.read()).decode("utf-8").strip()
        preview = await asyncio.to_thread(get_preview_from_url, question_text)
        print(f"Source: {preview.get('source', '[no source]')}")


        other_files = [
            v for k, v in form.items()
            if isinstance(v, StarletteUploadFile) and k != "questions.txt"
        ]
        if other_files:
            preview_from_files = try_preview_from_files(other_files)
            if preview_from_files:
                preview = preview_from_files


        if not preview or "error" in preview:
            preview = {}
       
        if "rendered_html" in preview:
            structured_preview = await generate_structured_preview_anthropic(preview["rendered_html"], question_text)
            print("Structured preview:", structured_preview)
            await asyncio.sleep(5)  # Allow time for the LLM to process
            llm_code = await generate_code(question_text, preview, html=structured_preview)
            print("Code by LLM:", llm_code)
        else:
            llm_code = await generate_code(question_text, preview, html=None)


        print("Code by LLM:", llm_code)
        cleaned = clean_code(llm_code)
        stdout, stderr = execute_code(cleaned)


        last_line = stdout.strip().splitlines()[-1]
        result = json.loads(last_line)


        return {
            "answer": result,
            "warnings": stderr.strip() if stderr else None
        }


    except Exception as e:
        return {
            "error": "Execution failed",
            "details": str(e),
            "stdout": stdout,
            "stderr": stderr
        }
</My Current untested code>, but im not sure how it works, or if it works